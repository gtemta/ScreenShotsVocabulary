import spacy
import nltk
from nltk.corpus import wordnet
from nltk.corpus import brown
import json
import os
import re

# ä¸‹è½½å¿…è¦çš„ NLTK æ•°æ®
nltk.download('wordnet')
nltk.download('brown')

def is_valid_english_word(word):
    """
    æ£€æŸ¥å•è¯æ˜¯å¦åªåŒ…å«è‹±æ–‡å­—æ¯ï¼Œå¹¶è¿‡æ»¤ç‰¹æ®Šæ ¼å¼
    
    Args:
        word (str): è¦æ£€æŸ¥çš„å•è¯
        
    Returns:
        bool: å¦‚æœæ˜¯æœ‰æ•ˆçš„è‹±æ–‡å­—æ¯åˆ™è¿”å› True
    """
    # è¿‡æ»¤æ‰åŒ…å«ç‰ˆæœ¬å·æ ¼å¼çš„æ–‡æœ¬ï¼ˆå¦‚ V0.92350ï¼‰
    if re.match(r'^[vV]\d+\.?\d*', word):
        return False
        
    # è¿‡æ»¤æ‰åŒ…å«æ•°å­—çš„æ–‡æœ¬
    if re.search(r'\d', word):
        return False
        
    # è¿‡æ»¤æ‰åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡æœ¬
    if re.search(r'[^a-zA-Z]', word):
        return False
        
    # è¿‡æ»¤æ‰å¤ªçŸ­çš„å•è¯
    if len(word) < 2:
        return False
        
    # è¿‡æ»¤æ‰å…¨å¤§å†™çš„å•è¯ï¼ˆé€šå¸¸æ˜¯ç¼©å†™ï¼‰
    if word.isupper():
        return False
        
    # è¿‡æ»¤æ‰åŒ…å«è¿ç»­ç›¸åŒå­—æ¯çš„å•è¯ï¼ˆå¦‚ "hellooo"ï¼‰
    if re.search(r'(.)\1{2,}', word):
        return False
        
    return True

class WordInfo:
    def __init__(self, word, chinese_word, definition, chinese_definition, examples, synonyms, antonyms):
        """
        å•å­—ä¿¡æ¯ç±»
        
        Args:
            word (str): è‹±æ–‡å•è¯
            chinese_word (str): ä¸­æ–‡ç¿»è¯‘
            definition (str): è‹±æ–‡å®šä¹‰
            chinese_definition (str): ä¸­æ–‡è§£é‡Š
            examples (list): ä¾‹å¥åˆ—è¡¨
            synonyms (list): è¿‘ä¹‰è¯åˆ—è¡¨
            antonyms (list): åä¹‰è¯åˆ—è¡¨
        """
        self.word = word
        self.chinese_word = chinese_word
        self.definition = definition
        self.chinese_definition = chinese_definition
        self.examples = examples if isinstance(examples, list) else [examples]
        self.synonyms = synonyms if isinstance(synonyms, list) else []
        self.antonyms = antonyms if isinstance(antonyms, list) else []

    def __str__(self):
        """ä»¥æ–‡å­—æ ¼å¼æ˜¾ç¤ºå•å­—ä¿¡æ¯"""
        return (
            f"ğŸ“– å•å­—: {self.word} ({self.chinese_word})\n"
            f"ğŸ” å®šä¹‰: {self.definition}\n"
            f"ğŸ“ ä¸­æ–‡è§£é‡Š: {self.chinese_definition}\n"
            f"ğŸ’¡ ä¾‹å¥: {'; '.join(self.examples)}\n"
            f"ğŸ”— è¿‘ä¹‰è¯: {', '.join(self.synonyms) if self.synonyms else 'æ— '}\n"
            f"ğŸš« åä¹‰è¯: {', '.join(self.antonyms) if self.antonyms else 'æ— '}"
        )

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆé€‚åˆ JSON å­˜å‚¨æˆ– API ä¼ è¾“ï¼‰"""
        return {
            "word": self.word,
            "chinese_word": self.chinese_word,
            "definition": self.definition,
            "chinese_definition": self.chinese_definition,
            "examples": self.examples,
            "synonyms": self.synonyms,
            "antonyms": self.antonyms
        }

    def to_markdown(self):
        """è½¬æ¢ä¸º Markdown æ ¼å¼"""
        return f"""# **{self.word.capitalize()} ({self.chinese_word})**

        ## ğŸ“– å®šä¹‰ (Definition)
        - {self.definition}

        ## ğŸ“ ä¸­æ–‡è§£é‡Š (Chinese Explanation)
        - {self.chinese_definition}

        ## ğŸ’¡ ä¾‹å¥ (Example Sentence)
        - {self.examples[0] if self.examples else 'No examples available'}

        ## ğŸ”— è¿‘ä¹‰è¯ (Synonyms)
        - {", ".join(self.synonyms) if self.synonyms else "æ— "}

        ## ğŸš« åä¹‰è¯ (Antonyms)
        - {", ".join(self.antonyms) if self.antonyms else "æ— "}
        """

def get_word_difficulty(word):
    """
    è¯„ä¼°å•è¯çš„éš¾åº¦
    
    Args:
        word (str): è¦è¯„ä¼°çš„å•è¯
        
    Returns:
        float: éš¾åº¦åˆ†æ•°ï¼ˆ0-1ä¹‹é—´ï¼Œè¶Šé«˜è¶Šéš¾ï¼‰
    """
    # è·å–å•è¯çš„æ‰€æœ‰åŒä¹‰è¯é›†
    synsets = wordnet.synsets(word)
    if not synsets:
        return 1.0  # å¦‚æœæ‰¾ä¸åˆ°åŒä¹‰è¯é›†ï¼Œè®¤ä¸ºæ˜¯æœ€éš¾çš„
        
    # è®¡ç®—éš¾åº¦åˆ†æ•°
    difficulty_score = 0.0
    
    # 1. åŸºäºè¯é¢‘çš„éš¾åº¦ï¼ˆåœ¨ Brown è¯­æ–™åº“ä¸­çš„å‡ºç°é¢‘ç‡ï¼‰
    word_freq = sum(1 for w in brown.words() if w.lower() == word.lower())
    freq_score = 1.0 - min(word_freq / 1000, 1.0)  # å½’ä¸€åŒ–åˆ° 0-1
    difficulty_score += freq_score * 0.4  # è¯é¢‘æƒé‡ 40%
    
    # 2. åŸºäºåŒä¹‰è¯æ•°é‡çš„éš¾åº¦
    synonym_count = len(set(lemma.name() for syn in synsets for lemma in syn.lemmas()))
    syn_score = min(synonym_count / 10, 1.0)  # å½’ä¸€åŒ–åˆ° 0-1
    difficulty_score += syn_score * 0.3  # åŒä¹‰è¯æ•°é‡æƒé‡ 30%
    
    # 3. åŸºäºå®šä¹‰é•¿åº¦çš„éš¾åº¦
    definition_length = sum(len(syn.definition().split()) for syn in synsets)
    def_score = min(definition_length / 50, 1.0)  # å½’ä¸€åŒ–åˆ° 0-1
    difficulty_score += def_score * 0.3  # å®šä¹‰é•¿åº¦æƒé‡ 30%
    
    return min(difficulty_score, 1.0)

def load_processed_words():
    """
    åŠ è½½å·²å¤„ç†è¿‡çš„å•è¯åˆ—è¡¨
    
    Returns:
        set: å·²å¤„ç†å•è¯é›†åˆ
    """
    try:
        if os.path.exists('processed_words.json'):
            with open('processed_words.json', 'r', encoding='utf-8') as f:
                return set(json.load(f))
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•åŠ è¼‰å·²è™•ç†å–®è©åˆ—è¡¨: {str(e)}")
    return set()

def save_processed_word(word):
    """
    ä¿å­˜å·²å¤„ç†çš„å•è¯
    
    Args:
        word (str): è¦ä¿å­˜çš„å•è¯
    """
    processed_words = load_processed_words()
    processed_words.add(word.lower())
    try:
        with open('processed_words.json', 'w', encoding='utf-8') as f:
            json.dump(list(processed_words), f, ensure_ascii=False)
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ä¿å­˜å·²è™•ç†å–®è©: {str(e)}")

def extract_keyword(text, min_difficulty=0.7):
    """
    ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼Œè¿‡æ»¤å·²å¤„ç†è¿‡çš„å•è¯å’Œç®€å•å•è¯
    
    Args:
        text (str): è¾“å…¥æ–‡æœ¬
        min_difficulty (float): æœ€å°éš¾åº¦è¦æ±‚ï¼ˆ0-1ä¹‹é—´ï¼‰ï¼Œé»˜è®¤0.7
        
    Returns:
        str: æå–çš„å…³é”®è¯
    """
    # åŠ è½½å·²å¤„ç†çš„å•è¯
    processed_words = load_processed_words()
    
    # åŠ è½½ spaCy æ¨¡å‹
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)
    
    # æå–æ‰€æœ‰å¯èƒ½çš„å•è¯
    candidates = []
    for token in doc:
        # åªè€ƒè™‘åè¯ã€åŠ¨è¯å’Œå½¢å®¹è¯
        if token.pos_ in ["NOUN", "VERB", "ADJ"]:
            word = token.text.lower()
            # è¿‡æ»¤å·²å¤„ç†çš„å•è¯ã€çŸ­å•è¯å’ŒåŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å•è¯
            if (word not in processed_words and 
                len(word) >= 2 and 
                is_valid_english_word(word)):
                # è®¡ç®—å•è¯éš¾åº¦
                difficulty = get_word_difficulty(word)
                if difficulty >= min_difficulty:
                    candidates.append((word, difficulty))
    
    if not candidates:
        return "No suitable keywords found"
    
    # æŒ‰éš¾åº¦æ’åºå¹¶é€‰æ‹©æœ€éš¾çš„å•è¯
    candidates.sort(key=lambda x: x[1], reverse=True)
    selected_word = candidates[0][0]
    
    # ä¿å­˜å·²å¤„ç†çš„å•è¯
    save_processed_word(selected_word)
    
    return selected_word

def get_word_info(word):
    """
    è·å–å•è¯çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        word (str): è¦æŸ¥è¯¢çš„å•è¯
        
    Returns:
        WordInfo: åŒ…å«å•è¯è¯¦ç»†ä¿¡æ¯çš„å¯¹è±¡
    """
    synsets = wordnet.synsets(word)
    if not synsets:
        return None

    definition = synsets[0].definition()
    examples = synsets[0].examples()[:1]
    synonyms = set(lemma.name() for syn in synsets for lemma in syn.lemmas())
    antonyms = set(lemma.antonyms()[0].name() for syn in synsets for lemma in syn.lemmas() if lemma.antonyms())

    word_info = WordInfo(
        word=word,
        chinese_word="",  # éœ€è¦å¦å¤–ç¿»è¯‘è·å–
        definition=definition,
        chinese_definition="",  # éœ€è¦å¦å¤–ç¿»è¯‘è·å–
        examples=examples if examples else ["No examples available"],
        synonyms=list(synonyms)[:3],
        antonyms=list(antonyms)[:3]
    )
    
    return word_info 