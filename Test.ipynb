{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è‹±æ–‡å–®å­—èˆ‡ç‰‡èªæå–å™¨\n",
    "\n",
    "é€™å€‹ notebook æ•´åˆäº†ä»¥ä¸‹åŠŸèƒ½ï¼š\n",
    "1. å¾åœ–ç‰‡ä¸­æå–æ–‡å­— (OCR)\n",
    "2. ä½¿ç”¨ OpenAI GPT åˆ†ææ–‡å­—ä¸¦æå–å­¸ç¿’å…§å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (1.82.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (11.2.1)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: spacy in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: nltk in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£å¿…è¦çš„å¥—ä»¶\n",
    "!pip install openai python-dotenv Pillow pytesseract spacy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yaoCat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\yaoCat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# ä¸‹è¼‰å¿…è¦çš„ NLTK æ•¸æ“š\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "\n",
    "# è¼‰å…¥ç’°å¢ƒè®Šé‡\n",
    "load_dotenv()\n",
    "\n",
    "# è¨­ç½® OpenAI API å¯†é‘°\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    å¾åœ–ç‰‡ä¸­æå–æ–‡å­—\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): åœ–ç‰‡è·¯å¾‘\n",
    "        \n",
    "    Returns:\n",
    "        str: æå–çš„æ–‡å­—å…§å®¹\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # é–‹å•Ÿåœ–ç‰‡\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # ä½¿ç”¨ pytesseract é€²è¡Œ OCR\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        \n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ OCR è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI GPT åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_learning_phrases_with_gpt(text: str, prompt: str = None) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ OpenAI GPT å¾æ–‡æœ¬ä¸­æå–é©åˆå­¸ç¿’çš„è‹±æ–‡å–®å­—å’Œç‰‡èª\n",
    "    \n",
    "    Args:\n",
    "        text (str): è¼¸å…¥çš„è‹±æ–‡æ–‡æœ¬\n",
    "        prompt (str, optional): è‡ªå®šç¾©æç¤ºè©ã€‚å¦‚æœæœªæä¾›ï¼Œå°‡ä½¿ç”¨é è¨­æç¤ºè©ã€‚\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, str]]: åŒ…å«æå–çš„å–®å­—/ç‰‡èªåŠå…¶è§£é‡‹çš„åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # å¦‚æœæœªæä¾›æç¤ºè©ï¼Œä½¿ç”¨é è¨­æç¤ºè©\n",
    "        if prompt is None:\n",
    "            prompt = f\"\"\"\n",
    "            Extract 3 useful English words or phrases from the following text for language learners.\n",
    "            For each entry, return:\n",
    "            1. The word or phrase\n",
    "            2. Its Chinese translation\n",
    "            3. A short explanation in simple English\n",
    "            4. An example sentence or usage\n",
    "\n",
    "            Please return the result in the following JSON format:\n",
    "            {{\n",
    "            \"phrases\": [\n",
    "            {{\n",
    "                \"phrase\": \"word or phrase\",\n",
    "                \"translation\": \"Chinese translation\",\n",
    "                \"explanation\": \"simple English explanation\",\n",
    "                \"example\": \"example sentence\"\n",
    "            }},\n",
    "            ...\n",
    "            ]\n",
    "            }}\n",
    "            \n",
    "            Please ensure the output is a valid JSON object, no extra commentary or explanation outside the JSON.\n",
    "            Text:\n",
    "            {text}\n",
    "            \"\"\"\n",
    "            \n",
    "        # èª¿ç”¨ OpenAI APIï¼ˆæ–°ç‰ˆæœ¬ï¼‰\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è‹±èªæ•™å­¸åŠ©æ‰‹ï¼Œæ“…é•·å¾æ–‡æœ¬ä¸­æå–é‡è¦çš„å­¸ç¿’å…§å®¹ã€‚\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # è§£æå›æ‡‰ï¼ˆæ–°ç‰ˆæœ¬ï¼‰\n",
    "        content = response.choices[0].message.content\n",
    "        # ç§»é™¤å¯èƒ½çš„ Markdown æ ¼å¼\n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]  # ç§»é™¤ ```json\n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]  # ç§»é™¤çµå°¾çš„ ```\n",
    "            \n",
    "        # æ¸…ç†å…§å®¹ä¸¦è§£æ JSON\n",
    "        content = content.strip()\n",
    "        result = json.loads(content)\n",
    "        \n",
    "        return result.get(\"phrases\", [])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æå–å­¸ç¿’å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(extracted_text: str) -> str:\n",
    "    return f\"\"\"\n",
    "Extract 3 useful English words or phrases from the following text for language learners.\n",
    "\n",
    "For each entry, return:\n",
    "1. The word or phrase\n",
    "2. Its Chinese translation\n",
    "3. A short explanation in simple English\n",
    "4. An example sentence or usage\n",
    "\n",
    "Please return the result in the following JSON format:\n",
    "{{\n",
    "  \"phrases\": [\n",
    "    {{\n",
    "      \"phrase\": \"word or phrase\",\n",
    "      \"translation\": \"Chinese translation\",\n",
    "      \"explanation\": \"simple English explanation\",\n",
    "      \"example\": \"example sentence\"\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Please ensure the output is a valid JSON object, no extra commentary or explanation outside the JSON.\n",
    "\n",
    "Text:\n",
    "{extracted_text}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_learning_content(text: str) -> Optional[Dict[str, List[Dict[str, str]]]]:\n",
    "    \"\"\"\n",
    "    æ•´åˆ GPT æå–çš„å­¸ç¿’å…§å®¹ï¼Œä¸¦é€²è¡Œå¾Œè™•ç†\n",
    "    \n",
    "    Args:\n",
    "        text (str): è¼¸å…¥çš„è‹±æ–‡æ–‡æœ¬\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Dict[str, List[Dict[str, str]]]]: è™•ç†å¾Œçš„å­¸ç¿’å…§å®¹\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ä½¿ç”¨ GPT æå–å­¸ç¿’å…§å®¹\n",
    "        prompt = build_prompt(text)\n",
    "        phrases = extract_learning_phrases_with_gpt(text, prompt)\n",
    "        \n",
    "        if not phrases:\n",
    "            return None\n",
    "            \n",
    "        # å°æ¯å€‹æå–çš„å…§å®¹é€²è¡Œé¡å¤–è™•ç†\n",
    "        processed_content = {\n",
    "            \"vocabulary\": [],\n",
    "            \"phrases\": []\n",
    "        }\n",
    "        \n",
    "        for item in phrases:\n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚ºå–®å­—ï¼ˆä¸åŒ…å«ç©ºæ ¼ï¼‰\n",
    "            if \" \" not in item[\"phrase\"]:\n",
    "                processed_content[\"vocabulary\"].append(item)\n",
    "            else:\n",
    "                processed_content[\"phrases\"].append(item)\n",
    "        \n",
    "        return processed_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è™•ç†å­¸ç¿’å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æå–çš„æ–‡å­—ï¼š\n",
      "Old Chronos warrants every bit of justice you dispensed and more, my niece! I was already angry with him after everything he'd done, and that was prior to my realizing that he'd unleashed terrifying Typhon on us all!\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# å¾åœ–ç‰‡æå–æ–‡å­—\n",
    "image_path = \"20240607222058_1.jpg\"  # æ›¿æ›ç‚ºæ‚¨çš„åœ–ç‰‡è·¯å¾‘\n",
    "# extracted_text = extract_text_from_image(image_path)\n",
    "extracted_text = \"Old Chronos warrants every bit of justice you dispensed and more, my niece! I was already angry with him after everything he'd done, and that was prior to my realizing that he'd unleashed terrifying Typhon on us all!\"\n",
    "print(\"æå–çš„æ–‡å­—ï¼š\")\n",
    "print(extracted_text)\n",
    "print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š å–®å­—ï¼š\n",
      "\n",
      "å–®å­—: warrants\n",
      "ç¿»è­¯: ä¿è­‰\n",
      "è§£é‡‹: to require or deserve\n",
      "ä¾‹å¥: His hard work warrants a promotion.\n",
      "\n",
      "å–®å­—: unleashed\n",
      "ç¿»è­¯: é‡‹æ”¾\n",
      "è§£é‡‹: to release or set free\n",
      "ä¾‹å¥: The company unleashed a new product into the market.\n",
      "\n",
      "ğŸ“ ç‰‡èªï¼š\n",
      "\n",
      "ç‰‡èª: prior to\n",
      "ç¿»è­¯: åœ¨...ä¹‹å‰\n",
      "è§£é‡‹: before a particular time or event\n",
      "ä¾‹å¥: I had never traveled abroad prior to this trip.\n"
     ]
    }
   ],
   "source": [
    "# åˆ†ææ–‡å­—ä¸¦æå–å­¸ç¿’å…§å®¹\n",
    "answer1 = get_learning_content(extracted_text)\n",
    "\n",
    "if answer1:\n",
    "    print(\"ğŸ“š å–®å­—ï¼š\")\n",
    "    for vocab in answer1[\"vocabulary\"]:\n",
    "        print(f\"\\nå–®å­—: {vocab['phrase']}\")\n",
    "        print(f\"ç¿»è­¯: {vocab['translation']}\")\n",
    "        print(f\"è§£é‡‹: {vocab['explanation']}\")\n",
    "        print(f\"ä¾‹å¥: {vocab['example']}\")\n",
    "    \n",
    "    print(\"\\nğŸ“ ç‰‡èªï¼š\")\n",
    "    for phrase in answer1[\"phrases\"]:\n",
    "        print(f\"\\nç‰‡èª: {phrase['phrase']}\")\n",
    "        print(f\"ç¿»è­¯: {phrase['translation']}\")\n",
    "        print(f\"è§£é‡‹: {phrase['explanation']}\")\n",
    "        print(f\"ä¾‹å¥: {phrase['example']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š å–®å­—ï¼š\n",
      "\n",
      "å–®å­—: warrants\n",
      "ç¿»è­¯: ä¿è­‰\n",
      "è§£é‡‹: è¡¨ç¤ºæ‡‰ç•¶æˆ–æ‡‰è©²ç™¼ç”Ÿçš„äº‹æƒ…\n",
      "ä¾‹å¥: The evidence warrants further investigation.\n",
      "\n",
      "å–®å­—: dispensed\n",
      "ç¿»è­¯: åˆ†ç™¼\n",
      "è§£é‡‹: åˆ†ç™¼æˆ–åˆ†é…æŸç‰©\n",
      "ä¾‹å¥: The nurse dispensed medicine to the patients.\n",
      "\n",
      "å–®å­—: unleashed\n",
      "ç¿»è­¯: é‡‹æ”¾\n",
      "è§£é‡‹: é‡‹æ”¾æˆ–å¼•ç™¼æŸç¨®åŠ›é‡æˆ–äº‹ç‰©\n",
      "ä¾‹å¥: The storm unleashed its fury on the coastal town.\n",
      "\n",
      "ğŸ“ ç‰‡èªï¼š\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if answer1:\n",
    "    print(\"ğŸ“š å–®å­—ï¼š\")\n",
    "    for vocab in answer1[\"vocabulary\"]:\n",
    "        print(f\"\\nå–®å­—: {vocab['phrase']}\")\n",
    "        print(f\"ç¿»è­¯: {vocab['translation']}\")\n",
    "        print(f\"è§£é‡‹: {vocab['explanation']}\")\n",
    "        print(f\"ä¾‹å¥: {vocab['example']}\")\n",
    "    \n",
    "    print(\"\\nğŸ“ ç‰‡èªï¼š\")\n",
    "    for phrase in answer1[\"phrases\"]:\n",
    "        print(f\"\\nç‰‡èª: {phrase['phrase']}\")\n",
    "        print(f\"ç¿»è­¯: {phrase['translation']}\")\n",
    "        print(f\"è§£é‡‹: {phrase['explanation']}\")\n",
    "        print(f\"ä¾‹å¥: {phrase['example']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocabulary': [{'phrase': 'warrants',\n",
       "   'translation': 'æ‡‰å¾—',\n",
       "   'explanation': 'to deserve or justify',\n",
       "   'example': 'He warrants a promotion for his hard work.'},\n",
       "  {'phrase': 'dispensed',\n",
       "   'translation': 'åˆ†ç™¼',\n",
       "   'explanation': 'to distribute or give out',\n",
       "   'example': 'The nurse dispensed medication to the patients.'},\n",
       "  {'phrase': 'unleashed',\n",
       "   'translation': 'é‡‹æ”¾',\n",
       "   'explanation': 'to release or set free',\n",
       "   'example': 'The storm unleashed its fury on the coast.'}],\n",
       " 'phrases': []}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Gemma \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def extract_learning_phrases_with_gemma(text: str, prompt: str = None) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Gemma å¾æ–‡æœ¬ä¸­æå–é©åˆå­¸ç¿’çš„è‹±æ–‡å–®å­—å’Œç‰‡èª\n",
    "    \n",
    "    Args:\n",
    "        text (str): è¼¸å…¥çš„è‹±æ–‡æ–‡æœ¬\n",
    "        prompt (str, optional): è‡ªå®šç¾©æç¤ºè©ã€‚å¦‚æœæœªæä¾›ï¼Œå°‡ä½¿ç”¨é è¨­æç¤ºè©ã€‚\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, str]]: åŒ…å«æå–çš„å–®å­—/ç‰‡èªåŠå…¶è§£é‡‹çš„åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # å¦‚æœæœªæä¾›æç¤ºè©ï¼Œä½¿ç”¨é è¨­æç¤ºè©\n",
    "        if prompt is None:\n",
    "            prompt = f\"\"\"\n",
    "            Extract at most 3 useful English words or phrases from the following text for medium class language learners.\n",
    "            For each entry, return:\n",
    "            1. The word or phrase\n",
    "            2. Its Chinese translation\n",
    "            3. A short explanation in simple English\n",
    "            4. An example sentence or usage\n",
    "\n",
    "            Please return the result in the following JSON format:\n",
    "            {{\n",
    "            \"phrases\": [\n",
    "            {{\n",
    "                \"phrase\": \"word or phrase\",\n",
    "                \"translation\": \"Chinese translation\",\n",
    "                \"explanation\": \"simple English explanation\",\n",
    "                \"example\": \"example sentence\"\n",
    "            }},\n",
    "            ...\n",
    "            ]\n",
    "            }}\n",
    "            \n",
    "            Please ensure the output is a valid JSON object, no extra commentary or explanation outside the JSON.\n",
    "            Text:\n",
    "            {text}\n",
    "            \"\"\"\n",
    "            \n",
    "        # èª¿ç”¨ OpenAI APIï¼ˆæ–°ç‰ˆæœ¬ï¼‰\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"gemma:2b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # è§£æå›æ‡‰ï¼ˆæ–°ç‰ˆæœ¬ï¼‰\n",
    "        raw = response.json()[\"response\"]\n",
    "        content = raw.strip()\n",
    "            \n",
    "        # æ¸…ç†å…§å®¹ä¸¦è§£æ JSON\n",
    "        content = content.strip()\n",
    "        result = json.loads(content)\n",
    "        \n",
    "        return result.get(\"phrases\", [])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æå–å­¸ç¿’å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_learning_content_gemma(text: str) -> Optional[Dict[str, List[Dict[str, str]]]]:\n",
    "    \"\"\"\n",
    "    æ•´åˆ Gemma æå–çš„å­¸ç¿’å…§å®¹ï¼Œä¸¦é€²è¡Œå¾Œè™•ç†\n",
    "    \n",
    "    Args:\n",
    "        text (str): è¼¸å…¥çš„è‹±æ–‡æ–‡æœ¬\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Dict[str, List[Dict[str, str]]]]: è™•ç†å¾Œçš„å­¸ç¿’å…§å®¹\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ä½¿ç”¨ GPT æå–å­¸ç¿’å…§å®¹\n",
    "        prompt = build_prompt(text)\n",
    "        phrases = extract_learning_phrases_with_gemma(text, prompt)\n",
    "        \n",
    "        if not phrases:\n",
    "            return None\n",
    "            \n",
    "        # å°æ¯å€‹æå–çš„å…§å®¹é€²è¡Œé¡å¤–è™•ç†\n",
    "        processed_content = {\n",
    "            \"vocabulary\": [],\n",
    "            \"phrases\": []\n",
    "        }\n",
    "        \n",
    "        for item in phrases:\n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚ºå–®å­—ï¼ˆä¸åŒ…å«ç©ºæ ¼ï¼‰\n",
    "            if \" \" not in item[\"phrase\"]:\n",
    "                processed_content[\"vocabulary\"].append(item)\n",
    "            else:\n",
    "                processed_content[\"phrases\"].append(item)\n",
    "        \n",
    "        return processed_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è™•ç†å­¸ç¿’å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(extracted_text)\n",
    "response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"gemma:2b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = response.json()[\"response\"]\n",
    "raw\n",
    "content = raw.strip()\n",
    "result = json.loads(content)\n",
    "answer2 = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š å–®å­—ï¼š\n",
      "\n",
      "å–®å­—: warrants\n",
      "ç¿»è­¯: ä¿è­‰\n",
      "è§£é‡‹: to require or deserve\n",
      "ä¾‹å¥: His hard work warrants a promotion.\n",
      "\n",
      "å–®å­—: unleashed\n",
      "ç¿»è­¯: é‡‹æ”¾\n",
      "è§£é‡‹: to release or set free\n",
      "ä¾‹å¥: The company unleashed a new product into the market.\n",
      "\n",
      "ğŸ“ ç‰‡èªï¼š\n",
      "\n",
      "ç‰‡èª: prior to\n",
      "ç¿»è­¯: åœ¨...ä¹‹å‰\n",
      "è§£é‡‹: before a particular time or event\n",
      "ä¾‹å¥: I had never traveled abroad prior to this trip.\n"
     ]
    }
   ],
   "source": [
    "# åˆ†ææ–‡å­—ä¸¦æå–å­¸ç¿’å…§å®¹\n",
    "answer2 = get_learning_content_gemma(extracted_text)\n",
    "\n",
    "if answer2:\n",
    "    print(\"ğŸ“š å–®å­—ï¼š\")\n",
    "    for vocab in answer2[\"vocabulary\"]:\n",
    "        print(f\"\\nå–®å­—: {vocab['phrase']}\")\n",
    "        print(f\"ç¿»è­¯: {vocab['translation']}\")\n",
    "        print(f\"è§£é‡‹: {vocab['explanation']}\")\n",
    "        print(f\"ä¾‹å¥: {vocab['example']}\")\n",
    "    \n",
    "    print(\"\\nğŸ“ ç‰‡èªï¼š\")\n",
    "    for phrase in answer2[\"phrases\"]:\n",
    "        print(f\"\\nç‰‡èª: {phrase['phrase']}\")\n",
    "        print(f\"ç¿»è­¯: {phrase['translation']}\")\n",
    "        print(f\"è§£é‡‹: {phrase['explanation']}\")\n",
    "        print(f\"ä¾‹å¥: {phrase['example']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = f\"\"\"\n",
    "You are an expert English tutor helping intermediate to advanced learners select the most effective vocabulary explanations.\n",
    "\n",
    "Below are two sets of vocabulary learning answers generated by different AI models. Each set includes:\n",
    "- 3 English words or phrases\n",
    "- Their Chinese translations\n",
    "- Short English explanations\n",
    "- Example sentences\n",
    "\n",
    "Evaluate both answers based on the following criteria:\n",
    "1. **Relevance** â€“ Are the chosen words/phrases useful and non-trivial for intermediate to advanced learners?\n",
    "2. **Accuracy** â€“ Are the explanations and translations correct and clearly understandable?\n",
    "3. **Clarity** â€“ Are the example sentences natural, relevant, and illustrative?\n",
    "4. **Depth** â€“ Does the explanation provide insight into real usage, including nuances?\n",
    "\n",
    "Please respond with:\n",
    "- Which answer (1 or 2) is better \n",
    "\n",
    "Here are the answers:\n",
    "\n",
    "Answer 1:\n",
    "{answer1}\n",
    "\n",
    "Answer 2:\n",
    "{answer2}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Bcw7RYd2NXdda8BkpIA3ImBsYg9A9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I would say that Answer 1 is better because:\\n\\n1. Relevance: The vocabulary words chosen (warrants, dispensed, unleashed) are more commonly used and relevant for intermediate to advanced learners compared to the phrases in Answer 2.\\n2. Accuracy: The translations and explanations in Answer 1 are clear and accurate, making it easier for learners to understand the words in context.\\n3. Clarity: The example sentences in Answer 1 are natural and illustrative, providing a clear picture of how the words are used in sentences.\\n4. Depth: Answer 1 provides insight into real usage by explaining the nuances of each word, which is important for learners looking to expand their vocabulary effectively.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1748619573, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=141, prompt_tokens=532, total_tokens=673, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_res = evaluation_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would say that Answer 1 is better because:\\n\\n1. Relevance: The vocabulary words chosen (warrants, dispensed, unleashed) are more commonly used and relevant for intermediate to advanced learners compared to the phrases in Answer 2.\\n2. Accuracy: The translations and explanations in Answer 1 are clear and accurate, making it easier for learners to understand the words in context.\\n3. Clarity: The example sentences in Answer 1 are natural and illustrative, providing a clear picture of how the words are used in sentences.\\n4. Depth: Answer 1 provides insight into real usage by explaining the nuances of each word, which is important for learners looking to expand their vocabulary effectively.'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
