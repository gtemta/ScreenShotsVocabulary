{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英文單字與片語提取器\n",
    "\n",
    "這個 notebook 整合了以下功能：\n",
    "1. 從圖片中提取文字 (OCR)\n",
    "2. 使用 OpenAI GPT 分析文字並提取學習內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (1.82.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (11.2.1)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: spacy in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: nltk in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yaocat\\documents\\github\\screenshotsvocabulary\\venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# 安裝必要的套件\n",
    "!pip install openai python-dotenv Pillow pytesseract spacy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yaoCat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\yaoCat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# 下載必要的 NLTK 數據\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "\n",
    "# 載入環境變量\n",
    "load_dotenv()\n",
    "\n",
    "# 設置 OpenAI API 密鑰\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR 功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    從圖片中提取文字\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 圖片路徑\n",
    "        \n",
    "    Returns:\n",
    "        str: 提取的文字內容\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 開啟圖片\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 使用 pytesseract 進行 OCR\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        \n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ OCR 處理時發生錯誤: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI GPT 功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_learning_phrases_with_gpt(text: str, prompt: str = None) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    使用 OpenAI GPT 從文本中提取適合學習的英文單字和片語\n",
    "    \n",
    "    Args:\n",
    "        text (str): 輸入的英文文本\n",
    "        prompt (str, optional): 自定義提示詞。如果未提供，將使用預設提示詞。\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, str]]: 包含提取的單字/片語及其解釋的列表\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 如果未提供提示詞，使用預設提示詞\n",
    "        if prompt is None:\n",
    "            prompt = f\"\"\"\n",
    "            Extract 3 useful English words or phrases from the following text for language learners.\n",
    "            For each entry, return:\n",
    "            1. The word or phrase\n",
    "            2. Its Chinese translation\n",
    "            3. A short explanation in simple English\n",
    "            4. An example sentence or usage\n",
    "\n",
    "            Please return the result in the following JSON format:\n",
    "            {{\n",
    "            \"phrases\": [\n",
    "            {{\n",
    "                \"phrase\": \"word or phrase\",\n",
    "                \"translation\": \"Chinese translation\",\n",
    "                \"explanation\": \"simple English explanation\",\n",
    "                \"example\": \"example sentence\"\n",
    "            }},\n",
    "            ...\n",
    "            ]\n",
    "            }}\n",
    "            \n",
    "            Please ensure the output is a valid JSON object, no extra commentary or explanation outside the JSON.\n",
    "            Text:\n",
    "            {text}\n",
    "            \"\"\"\n",
    "            \n",
    "        # 調用 OpenAI API（新版本）\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"你是一個專業的英語教學助手，擅長從文本中提取重要的學習內容。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # 解析回應（新版本）\n",
    "        content = response.choices[0].message.content\n",
    "        # 移除可能的 Markdown 格式\n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]  # 移除 ```json\n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]  # 移除結尾的 ```\n",
    "            \n",
    "        # 清理內容並解析 JSON\n",
    "        content = content.strip()\n",
    "        result = json.loads(content)\n",
    "        \n",
    "        return result.get(\"phrases\", [])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 提取學習內容時發生錯誤: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(extracted_text: str) -> str:\n",
    "    return f\"\"\"\n",
    "Extract 3 useful English words or phrases from the following text for language learners.\n",
    "\n",
    "For each entry, return:\n",
    "1. The word or phrase\n",
    "2. Its Chinese translation\n",
    "3. A short explanation in simple English\n",
    "4. An example sentence or usage\n",
    "\n",
    "Please return the result in the following JSON format:\n",
    "{{\n",
    "  \"phrases\": [\n",
    "    {{\n",
    "      \"phrase\": \"word or phrase\",\n",
    "      \"translation\": \"Chinese translation\",\n",
    "      \"explanation\": \"simple English explanation\",\n",
    "      \"example\": \"example sentence\"\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Please ensure the output is a valid JSON object, no extra commentary or explanation outside the JSON.\n",
    "\n",
    "Text:\n",
    "{extracted_text}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_learning_content(text: str) -> Optional[Dict[str, List[Dict[str, str]]]]:\n",
    "    \"\"\"\n",
    "    整合 GPT 提取的學習內容，並進行後處理\n",
    "    \n",
    "    Args:\n",
    "        text (str): 輸入的英文文本\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Dict[str, List[Dict[str, str]]]]: 處理後的學習內容\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 使用 GPT 提取學習內容\n",
    "        prompt = build_prompt(text)\n",
    "        phrases = extract_learning_phrases_with_gpt(text, prompt)\n",
    "        \n",
    "        if not phrases:\n",
    "            return None\n",
    "            \n",
    "        # 對每個提取的內容進行額外處理\n",
    "        processed_content = {\n",
    "            \"vocabulary\": [],\n",
    "            \"phrases\": []\n",
    "        }\n",
    "        \n",
    "        for item in phrases:\n",
    "            # 檢查是否為單字（不包含空格）\n",
    "            if \" \" not in item[\"phrase\"]:\n",
    "                processed_content[\"vocabulary\"].append(item)\n",
    "            else:\n",
    "                processed_content[\"phrases\"].append(item)\n",
    "        \n",
    "        return processed_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 處理學習內容時發生錯誤: {str(e)}\")\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取的文字：\n",
      "Old Chronos warrants every bit of justice you dispensed and more, my niece! I was already angry with him after everything he'd done, and that was prior to my realizing that he'd unleashed terrifying Typhon on us all!\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 從圖片提取文字\n",
    "image_path = \"20240607222058_1.jpg\"  # 替換為您的圖片路徑\n",
    "# extracted_text = extract_text_from_image(image_path)\n",
    "extracted_text = \"Old Chronos warrants every bit of justice you dispensed and more, my niece! I was already angry with him after everything he'd done, and that was prior to my realizing that he'd unleashed terrifying Typhon on us all!\"\n",
    "print(\"提取的文字：\")\n",
    "print(extracted_text)\n",
    "print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 單字：\n",
      "\n",
      "單字: warrants\n",
      "翻譯: 保證\n",
      "解釋: to require or deserve\n",
      "例句: His hard work warrants a promotion.\n",
      "\n",
      "單字: unleashed\n",
      "翻譯: 釋放\n",
      "解釋: to release or set free\n",
      "例句: The company unleashed a new product into the market.\n",
      "\n",
      "📝 片語：\n",
      "\n",
      "片語: prior to\n",
      "翻譯: 在...之前\n",
      "解釋: before a particular time or event\n",
      "例句: I had never traveled abroad prior to this trip.\n"
     ]
    }
   ],
   "source": [
    "# 分析文字並提取學習內容\n",
    "answer1 = get_learning_content(extracted_text)\n",
    "\n",
    "if answer1:\n",
    "    print(\"📚 單字：\")\n",
    "    for vocab in answer1[\"vocabulary\"]:\n",
    "        print(f\"\\n單字: {vocab['phrase']}\")\n",
    "        print(f\"翻譯: {vocab['translation']}\")\n",
    "        print(f\"解釋: {vocab['explanation']}\")\n",
    "        print(f\"例句: {vocab['example']}\")\n",
    "    \n",
    "    print(\"\\n📝 片語：\")\n",
    "    for phrase in answer1[\"phrases\"]:\n",
    "        print(f\"\\n片語: {phrase['phrase']}\")\n",
    "        print(f\"翻譯: {phrase['translation']}\")\n",
    "        print(f\"解釋: {phrase['explanation']}\")\n",
    "        print(f\"例句: {phrase['example']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 單字：\n",
      "\n",
      "單字: warrants\n",
      "翻譯: 保證\n",
      "解釋: 表示應當或應該發生的事情\n",
      "例句: The evidence warrants further investigation.\n",
      "\n",
      "單字: dispensed\n",
      "翻譯: 分發\n",
      "解釋: 分發或分配某物\n",
      "例句: The nurse dispensed medicine to the patients.\n",
      "\n",
      "單字: unleashed\n",
      "翻譯: 釋放\n",
      "解釋: 釋放或引發某種力量或事物\n",
      "例句: The storm unleashed its fury on the coastal town.\n",
      "\n",
      "📝 片語：\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if answer1:\n",
    "    print(\"📚 單字：\")\n",
    "    for vocab in answer1[\"vocabulary\"]:\n",
    "        print(f\"\\n單字: {vocab['phrase']}\")\n",
    "        print(f\"翻譯: {vocab['translation']}\")\n",
    "        print(f\"解釋: {vocab['explanation']}\")\n",
    "        print(f\"例句: {vocab['example']}\")\n",
    "    \n",
    "    print(\"\\n📝 片語：\")\n",
    "    for phrase in answer1[\"phrases\"]:\n",
    "        print(f\"\\n片語: {phrase['phrase']}\")\n",
    "        print(f\"翻譯: {phrase['translation']}\")\n",
    "        print(f\"解釋: {phrase['explanation']}\")\n",
    "        print(f\"例句: {phrase['example']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocabulary': [{'phrase': 'warrants',\n",
       "   'translation': '應得',\n",
       "   'explanation': 'to deserve or justify',\n",
       "   'example': 'He warrants a promotion for his hard work.'},\n",
       "  {'phrase': 'dispensed',\n",
       "   'translation': '分發',\n",
       "   'explanation': 'to distribute or give out',\n",
       "   'example': 'The nurse dispensed medication to the patients.'},\n",
       "  {'phrase': 'unleashed',\n",
       "   'translation': '釋放',\n",
       "   'explanation': 'to release or set free',\n",
       "   'example': 'The storm unleashed its fury on the coast.'}],\n",
       " 'phrases': []}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Gemma \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def extract_learning_phrases_with_gemma(text: str, prompt: str = None) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    使用 Gemma 從文本中提取適合學習的英文單字和片語\n",
    "    \n",
    "    Args:\n",
    "        text (str): 輸入的英文文本\n",
    "        prompt (str, optional): 自定義提示詞。如果未提供，將使用預設提示詞。\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, str]]: 包含提取的單字/片語及其解釋的列表\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 如果未提供提示詞，使用預設提示詞\n",
    "        if prompt is None:\n",
    "            prompt = f\"\"\"\n",
    "            Extract at most 3 useful English words or phrases from the following text for medium class language learners.\n",
    "            For each entry, return:\n",
    "            1. The word or phrase\n",
    "            2. Its Chinese translation\n",
    "            3. A short explanation in simple English\n",
    "            4. An example sentence or usage\n",
    "\n",
    "            Please return the result in the following JSON format:\n",
    "            {{\n",
    "            \"phrases\": [\n",
    "            {{\n",
    "                \"phrase\": \"word or phrase\",\n",
    "                \"translation\": \"Chinese translation\",\n",
    "                \"explanation\": \"simple English explanation\",\n",
    "                \"example\": \"example sentence\"\n",
    "            }},\n",
    "            ...\n",
    "            ]\n",
    "            }}\n",
    "            \n",
    "            Please ensure the output is a valid JSON object, no extra commentary or explanation outside the JSON.\n",
    "            Text:\n",
    "            {text}\n",
    "            \"\"\"\n",
    "            \n",
    "        # 調用 OpenAI API（新版本）\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"gemma:2b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 解析回應（新版本）\n",
    "        raw = response.json()[\"response\"]\n",
    "        content = raw.strip()\n",
    "            \n",
    "        # 清理內容並解析 JSON\n",
    "        content = content.strip()\n",
    "        result = json.loads(content)\n",
    "        \n",
    "        return result.get(\"phrases\", [])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 提取學習內容時發生錯誤: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_learning_content_gemma(text: str) -> Optional[Dict[str, List[Dict[str, str]]]]:\n",
    "    \"\"\"\n",
    "    整合 Gemma 提取的學習內容，並進行後處理\n",
    "    \n",
    "    Args:\n",
    "        text (str): 輸入的英文文本\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Dict[str, List[Dict[str, str]]]]: 處理後的學習內容\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 使用 GPT 提取學習內容\n",
    "        prompt = build_prompt(text)\n",
    "        phrases = extract_learning_phrases_with_gemma(text, prompt)\n",
    "        \n",
    "        if not phrases:\n",
    "            return None\n",
    "            \n",
    "        # 對每個提取的內容進行額外處理\n",
    "        processed_content = {\n",
    "            \"vocabulary\": [],\n",
    "            \"phrases\": []\n",
    "        }\n",
    "        \n",
    "        for item in phrases:\n",
    "            # 檢查是否為單字（不包含空格）\n",
    "            if \" \" not in item[\"phrase\"]:\n",
    "                processed_content[\"vocabulary\"].append(item)\n",
    "            else:\n",
    "                processed_content[\"phrases\"].append(item)\n",
    "        \n",
    "        return processed_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 處理學習內容時發生錯誤: {str(e)}\")\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(extracted_text)\n",
    "response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"gemma:2b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = response.json()[\"response\"]\n",
    "raw\n",
    "content = raw.strip()\n",
    "result = json.loads(content)\n",
    "answer2 = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 單字：\n",
      "\n",
      "單字: warrants\n",
      "翻譯: 保證\n",
      "解釋: to require or deserve\n",
      "例句: His hard work warrants a promotion.\n",
      "\n",
      "單字: unleashed\n",
      "翻譯: 釋放\n",
      "解釋: to release or set free\n",
      "例句: The company unleashed a new product into the market.\n",
      "\n",
      "📝 片語：\n",
      "\n",
      "片語: prior to\n",
      "翻譯: 在...之前\n",
      "解釋: before a particular time or event\n",
      "例句: I had never traveled abroad prior to this trip.\n"
     ]
    }
   ],
   "source": [
    "# 分析文字並提取學習內容\n",
    "answer2 = get_learning_content_gemma(extracted_text)\n",
    "\n",
    "if answer2:\n",
    "    print(\"📚 單字：\")\n",
    "    for vocab in answer2[\"vocabulary\"]:\n",
    "        print(f\"\\n單字: {vocab['phrase']}\")\n",
    "        print(f\"翻譯: {vocab['translation']}\")\n",
    "        print(f\"解釋: {vocab['explanation']}\")\n",
    "        print(f\"例句: {vocab['example']}\")\n",
    "    \n",
    "    print(\"\\n📝 片語：\")\n",
    "    for phrase in answer2[\"phrases\"]:\n",
    "        print(f\"\\n片語: {phrase['phrase']}\")\n",
    "        print(f\"翻譯: {phrase['translation']}\")\n",
    "        print(f\"解釋: {phrase['explanation']}\")\n",
    "        print(f\"例句: {phrase['example']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = f\"\"\"\n",
    "You are an expert English tutor helping intermediate to advanced learners select the most effective vocabulary explanations.\n",
    "\n",
    "Below are two sets of vocabulary learning answers generated by different AI models. Each set includes:\n",
    "- 3 English words or phrases\n",
    "- Their Chinese translations\n",
    "- Short English explanations\n",
    "- Example sentences\n",
    "\n",
    "Evaluate both answers based on the following criteria:\n",
    "1. **Relevance** – Are the chosen words/phrases useful and non-trivial for intermediate to advanced learners?\n",
    "2. **Accuracy** – Are the explanations and translations correct and clearly understandable?\n",
    "3. **Clarity** – Are the example sentences natural, relevant, and illustrative?\n",
    "4. **Depth** – Does the explanation provide insight into real usage, including nuances?\n",
    "\n",
    "Please respond with:\n",
    "- Which answer (1 or 2) is better \n",
    "\n",
    "Here are the answers:\n",
    "\n",
    "Answer 1:\n",
    "{answer1}\n",
    "\n",
    "Answer 2:\n",
    "{answer2}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Bcw7RYd2NXdda8BkpIA3ImBsYg9A9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I would say that Answer 1 is better because:\\n\\n1. Relevance: The vocabulary words chosen (warrants, dispensed, unleashed) are more commonly used and relevant for intermediate to advanced learners compared to the phrases in Answer 2.\\n2. Accuracy: The translations and explanations in Answer 1 are clear and accurate, making it easier for learners to understand the words in context.\\n3. Clarity: The example sentences in Answer 1 are natural and illustrative, providing a clear picture of how the words are used in sentences.\\n4. Depth: Answer 1 provides insight into real usage by explaining the nuances of each word, which is important for learners looking to expand their vocabulary effectively.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1748619573, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=141, prompt_tokens=532, total_tokens=673, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_res = evaluation_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would say that Answer 1 is better because:\\n\\n1. Relevance: The vocabulary words chosen (warrants, dispensed, unleashed) are more commonly used and relevant for intermediate to advanced learners compared to the phrases in Answer 2.\\n2. Accuracy: The translations and explanations in Answer 1 are clear and accurate, making it easier for learners to understand the words in context.\\n3. Clarity: The example sentences in Answer 1 are natural and illustrative, providing a clear picture of how the words are used in sentences.\\n4. Depth: Answer 1 provides insight into real usage by explaining the nuances of each word, which is important for learners looking to expand their vocabulary effectively.'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
